{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here. A good way to derive solutions for these tasks is to derive it for single elements and then generalize to the resulting matrix/vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar w.r.t. vector:\n",
    "$$  \n",
    "y = c^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let: \n",
    "$$x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\dots \\\\ x_N  \\end{bmatrix}$$\n",
    "$$c^T = \\begin{bmatrix} c_1, c_2, \\dots , c_N \\end{bmatrix}$$\n",
    "\n",
    "* Now:\n",
    "$$y = c^T x = \\sum\\limits_i c_i x_i \\rightarrow \\frac{dy}{dx_i} = c_i$$\n",
    "$$\\frac{dy}{dx} = \\begin{bmatrix}\\frac{dy}{dx_1} & \\dots & \\frac{dy}{dx_N}\\end{bmatrix} = \\begin{bmatrix}c_1 & c_2 & \\dots & c_N \\end{bmatrix} = c^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector w.r.t. vector:\n",
    "$$ y = \\sum_{j=1}^{N} cx^T \\quad c \\in \\mathbb{R}^{M} ,x \\in \\mathbb{R}^{N}, cx^T \\in \\mathbb{R}^{M \\times N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Like in previous one:\n",
    "$$c = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\dots \\\\c_M \\end{bmatrix} $$\n",
    "$$x^T = \\begin{bmatrix} x_1 & x_2 & \\dots & x_N \\end{bmatrix}$$\n",
    "\n",
    "* Now:\n",
    "$$cx^T = \\begin{bmatrix} c_1 x_1 & c_1 x_2 & \\dots &c_1 x_{N-1}& c_1 x_N \\\\\n",
    "c_2 x_1 & c_2 x_2 & \\dots & c_2 x_{N-1} & c_2 x_N \\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "c_M x_1 & c_M x_2 & \\dots & c_M x_{N-1} &  c_M x_N\n",
    "\\end{bmatrix} \\rightarrow  y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\dots \\\\ y_M \\end{bmatrix} =  \\begin{bmatrix} с_1 \\sum\\limits_{j = 1}^N x_i\\\\ с_2 \\sum\\limits_{j = 1}^N x_i\\\\ с_3 \\sum\\limits_{j = 1}^N x_i\\\\ \\dots\\\\ c_M \\sum\\limits_{j = 1}^N x_i   \\\\ \\end{bmatrix}$$\n",
    "\n",
    "* Then deriative $\\frac{dy}{dx}$ is a matrix:\n",
    "$$\\frac{dy}{dx}  = \\begin{bmatrix} \\frac{dy_1}{dx_1} & \\dots &\\frac{dy_1}{dx_N}\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\frac{dy_M}{dx_1} & \\dots & \\frac{dy_M}{dx_N}\n",
    "\\end{bmatrix} = \\underbrace{\\begin{bmatrix} c_1 & \\dots & c_1\\\\\n",
    "c_2 & \\dots & c_2\\\\\n",
    "\\dots& \\dots & \\dots \\\\\n",
    "c_{M-1} & \\dots & c_{M-1}\\\\\n",
    "c_M & \\dots & c_M \\\\ \\end{bmatrix}}_{N \\text{similar columns}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector w.r.t. vector:\n",
    "$$  \n",
    "y = x x^T x , x \\in \\mathbb{R}^{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Let's assume $x$, that column vector:\n",
    "$$ x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\dots\\\\ x_N \\\\ \\end{bmatrix} $$\n",
    "* For $y$ we can write down:\n",
    "$$y = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\dots \\\\y_{N - 1} \\\\ y_N \\end{bmatrix} =  x x^T x = \\begin{bmatrix} x_1 \\sum \\limits_i^N x_i^2 \\\\ x_2 \\sum \\limits_i^N x_i^2 \\\\ \\dots \\\\ x_{N-1} \\sum \\limits_i^N x_i^2 \\\\ x_N \\sum \\limits_i^N x_i^2 \\end{bmatrix}$$\n",
    "\n",
    "* Similar to previous task:\n",
    "$$\\frac{dy}{dx}  = \\begin{bmatrix} \\frac{dy_1}{dx_1} & \\dots &\\frac{dy_1}{dx_N}\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\frac{dy_N}{dx_1} & \\dots & \\frac{dy_N}{dx_N}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "* Now we need to find a derivation: \n",
    "$$\\frac{dy_j}{dx_k} = \\frac{d(x_j \\sum \\limits_i^N x_i^2) }{dx_k} = \\begin{cases} \\sum \\limits_i^N x_i^2  + 2 x_j x_k \\ \\ \\ \\text{if $j = k$} \\\\\n",
    "2 x_j x_k \\ \\ \\ \\text{if $j \\neq k$} \\\\\n",
    "\\end{cases}  $$\n",
    "\n",
    "* Finally: \n",
    "$$\\frac{dy}{dx} =  \\begin{bmatrix} \\frac{dy_1}{dx_1} & \\dots &\\frac{dy_1}{dx_N}\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\frac{dy_N}{dx_1} & \\dots & \\frac{dy_N}{dx_N}\n",
    "\\end{bmatrix} =  \\begin{bmatrix} 2 x_1 x_1 +  \\sum \\limits_i^N x_i^2 & \\dots & 2 x_1 x_N\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "2 x_N x_1 & \\dots & 2 x_N x_N +  \\sum \\limits_i^N x_i^2\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "* As we know $ \\sum \\limits_i^N x_i^2 = x^T x$ this elements stand only are diagonal elements, so:\n",
    "$$\\frac{dy}{dx}  =  \\begin{bmatrix} 2 x_1 x_1 & \\dots & 2 x_1 x_N\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "2 x_N x_1 & \\dots & 2 x_N x_N \n",
    "\\end{bmatrix} + \\text{diag}\\begin{pmatrix}\\sum \\limits_i^N x_i^2 , \\dots, \\sum \\limits_i^N x_i^2 \\end{pmatrix} = \\begin{bmatrix} 2 x_1 x_1 & \\dots & 2 x_1 x_N\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "2 x_N x_1 & \\dots & 2 x_N x_N \n",
    "\\end{bmatrix} + x^T x \\mathbf{E}, $$\n",
    "where $\\mathbf{E}$ is identity matrix of size $N \\times N$\n",
    "* What's more for first matrix we can write:\n",
    "$$\\begin{bmatrix} 2 x_1 x_1 & \\dots & 2 x_1 x_N\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "2 x_N x_1 & \\dots & 2 x_N x_N \n",
    "\\end{bmatrix} = 2x x^T  \\rightarrow \\frac{dy}{dx} =x^T x \\mathbf{E} +  2x x^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives for the parameters of the Dense layer:\n",
    "\n",
    "***Given :***  $$Y = XW, Y \\in \\mathbb{R}^{N \\times OUT}, X \\in \\mathbb{R}^{N \\times IN}, W \\in \\mathbb{R}^{IN \\times OUT} $$ \n",
    "\n",
    "The derivative of the hypothetic loss function w.r.t. to $Y$ is known: $\\Delta Y  \\in \\mathbb{R}^{N \\times OUT}$\n",
    "\n",
    "***Task :*** Please, derive the gradients of the loss w.r.t the weight matrix $W$: $\\Delta W  \\in \\mathbb{R}^{IN \\times OUT}$. Use the chain rule. First, please, derive each element of the $\\Delta W$, then generalize to the matrix form.\n",
    " \n",
    "Useful link: http://cs231n.stanford.edu/vecDerivs.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Let's find the derivative of the hypothetic loss (denoted ad $\\mathcal{L}$), from which we receive that:\n",
    "$$\\Delta W = \\frac{\\partial \\mathcal{L}}{\\partial W} \\\\$$\n",
    "$$\\Delta Y = \\frac{\\partial \\mathcal{L}}{\\partial Y}$$\n",
    "\n",
    "* Now let's use the chain rule for $\\Delta W$:\n",
    "$$\\Delta W = \\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{\\partial \\mathcal{L}}{\\partial Y} \\frac{\\partial Y}{\\partial W} =\\Delta Y \\frac{\\partial Y}{\\partial W} \\\\ $$\n",
    "\n",
    "* Now let's find the  element-wise representation of $\\Delta W$, denoted as $\\Delta w_{ij}$:\n",
    "$$\\Delta w_{ij} = \\sum_{i=1}^{N} \\Delta y_{ij} \\frac{\\partial y_{ij}}{\\partial w_{ij}}$$\n",
    "* Now let's look at $\\frac{\\partial y_{ij}}{\\partial w_{ij}}$.\n",
    "* From the given above formula:\n",
    "$$Y = X W \\rightarrow  X = \\frac{\\partial Y}{\\partial W}$$\n",
    "or element-wise:\n",
    "$$ x_{ij} = \\frac{\\partial y_{ij}}{\\partial w_{ij}} $$\n",
    "* So returning to  $\\Delta w_{ij}$:\n",
    "$$\\Delta w_{ij} = \\sum_{i=1}^{N} \\Delta y_{ij} \\frac{\\partial y_{ij}}{\\partial w_{ij}} \\rightarrow \\Delta w_{ij} = \\sum_{i=1}^{N} \\Delta y_{ij} x_{ij} \\rightarrow \\Delta W = X^T \\Delta Y$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
