{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization used for this homework is based on Alexandr Verinov's code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will try several criterions for learning an implicit model. Almost everything is written for you, and you only need to implement the objective for the game and play around with the model. \n",
    "\n",
    "**0)** Read the code\n",
    "\n",
    "**1)** Implement objective for a vanilla [Generative Adversarial Networks](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) (GAN). The hyperparameters are already set in the code. The model will converge if you implement the objective (1) right. \n",
    "\n",
    "**2)** Note the discussion in the paper, that the objective for $G$ can be of two kinds: $min_G log(1 - D)$ and $min_G - log(D)$. Implement the second objective and ensure model converges. Most likely, in this example you will not notice the difference, but people usually use the second objective, it really matters in more complicated scenarios.\n",
    "\n",
    "**3 & 4)** Implement [Wasserstein GAN](https://arxiv.org/abs/1701.07875) ([WGAN](https://arxiv.org/abs/1704.00028)) and WGAN-GP. To make the discriminator have Lipschitz property you need to clip discriminator's weights to $[-0.01, 0.01]$ range (WGAN) or use gradient penalty (WGAN-GP). You will need to make few modifications to the code: 1) remove sigmoids from discriminator 2) add weight clipping clipping / gradient penaly. 3) change objective. See [implementation 1](https://github.com/martinarjovsky/WassersteinGAN/) / [implementation 2](https://github.com/caogang/wgan-gp). They also use different optimizer. The default hyperparameters may not work, spend time to tune them.\n",
    "\n",
    "**5) Bonus: same thing without GANs** Implement maximum mean discrepancy estimator (MMD). MMD is discrepancy measure between distributions. In our case we use it to calculate discrepancy between real and fake data. You need to implement RBF kernel $k(x,x')=\\exp \\left(-{\\frac  {1}{2\\sigma ^{2}}}||x-x'||^{2}\\right)$ and an MMD estimator (see eq.8 from https://arxiv.org/pdf/1505.03906.pdf). MMD is then used instead of discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Please, implement everything in one notebook, using if statements to switch between the tasks\n",
    "\"\"\"\n",
    "TASK = 1 # 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12345)\n",
    "lims=(-5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define sampler from real data and Z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_discrete\n",
    "\n",
    "MEANS = np.array(\n",
    "        [[-1,-3],\n",
    "         [1,3],\n",
    "         [-2,0],\n",
    "        ])\n",
    "COVS = np.array(\n",
    "        [[[1,0.8],[0.8,1]],\n",
    "        [[1,-0.5],[-0.5,1]],\n",
    "        [[1,0],[0,1]],\n",
    "        ])\n",
    "PROBS = np.array([\n",
    "        0.2,\n",
    "        0.5,\n",
    "        0.3\n",
    "        ])\n",
    "assert len(MEANS) == len(COVS) == len(PROBS), \"number of components mismatch\"\n",
    "COMPONENTS = len(MEANS)\n",
    "\n",
    "comps_dist = rv_discrete(values=(range(COMPONENTS), PROBS))\n",
    "\n",
    "def sample_true(N):\n",
    "    comps = comps_dist.rvs(size=N)\n",
    "    conds = np.arange(COMPONENTS)[:,None] == comps[None,:]\n",
    "    arr = np.array([np.random.multivariate_normal(MEANS[c], COVS[c], size=N)\n",
    "                     for c in range(COMPONENTS)])\n",
    "    return np.select(conds[:,:,None], arr).astype(np.float32)\n",
    "\n",
    "NOISE_DIM = 20\n",
    "def sample_noise(N):\n",
    "    return np.random.normal(size=(N,NOISE_DIM)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_data(data):\n",
    "    \"\"\"\n",
    "        Visualizes data as histogram\n",
    "    \"\"\"\n",
    "    hist = np.histogram2d(data[:, 1], data[:, 0], bins=100, range=[lims, lims])\n",
    "    plt.pcolormesh(hist[1], hist[2], hist[0], alpha=0.5)\n",
    "\n",
    "fixed_noise = sample_noise(1000)\n",
    "def vis_g():\n",
    "    \"\"\"\n",
    "        Visualizes generator's samples as circles\n",
    "    \"\"\"\n",
    "    data = generator(Variable(torch.Tensor(fixed_noise))).data.numpy()\n",
    "    if np.isnan(data).any():\n",
    "        return\n",
    "    \n",
    "    plt.scatter(data[:,0], data[:,1], alpha=0.2, c='b')\n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    \n",
    "def vis_d():\n",
    "    \"\"\"\n",
    "        Visualizes discriminator's gradient on grid\n",
    "    \"\"\"\n",
    "    X, Y = np.meshgrid(np.linspace(lims[0], lims[1], 30), np.linspace(lims[0], lims[1], 30))\n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "    grid = Variable(torch.Tensor(np.vstack([X, Y]).T), requires_grad=True)\n",
    "    data_gen = generator(Variable(torch.Tensor(fixed_noise)))\n",
    "    loss = d_loss(discriminator(data_gen), discriminator(grid))\n",
    "    loss.backward()\n",
    "    grads = - grid.grad.data.numpy()\n",
    "    plt.quiver(X, Y, grads[:, 0], grads[:, 1], color='black',alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've passed task 1 you can play with architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, out_dim, hidden_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(noise_dim, hidden_dim)\n",
    "        nn.init.xavier_normal(self.fc1.weight)\n",
    "        nn.init.constant(self.fc1.bias, 0.0)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        nn.init.xavier_normal(self.fc2.weight)\n",
    "        nn.init.constant(self.fc2.bias, 0.0)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
    "        nn.init.xavier_normal(self.fc3.weight)\n",
    "        nn.init.constant(self.fc3.bias, 0.0)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "            Generator takes a vector of noise and produces sample\n",
    "        \"\"\"\n",
    "        h1 = F.tanh(self.fc1(z))\n",
    "        h2 = F.leaky_relu(self.fc2(h1))\n",
    "        y_gen = self.fc3(h2)\n",
    "        return y_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=100):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        nn.init.xavier_normal(self.fc1.weight)\n",
    "        nn.init.constant(self.fc1.bias, 0.0)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        nn.init.xavier_normal(self.fc2.weight)\n",
    "        nn.init.constant(self.fc2.bias, 0.0)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        nn.init.xavier_normal(self.fc3.weight)\n",
    "        nn.init.constant(self.fc3.bias, 0.0)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "        nn.init.xavier_normal(self.fc4.weight)\n",
    "        nn.init.constant(self.fc4.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.tanh(self.fc1(x))\n",
    "        h2 = F.leaky_relu(self.fc2(h1))\n",
    "        h3 = F.leaky_relu(self.fc3(h2))\n",
    "        score = F.sigmoid(self.fc4(h3))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define updates and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(NOISE_DIM, out_dim = 2)\n",
    "discriminator = Discriminator(in_dim = 2)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(),     lr=lr, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are using ADAM optimizer with `beta1=0.5` for both discriminator and discriminator. This is a common practice and works well. Motivation: models should be flexible and adapt itself rapidly to the distributions. \n",
    "\n",
    "You can try different optimizers and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# IMPLEMENT HERE\n",
    "# Define the g_loss and d_loss here\n",
    "# these are the only lines of code you need to change to implement GAN game\n",
    "\n",
    "def g_loss():\n",
    "    # if TASK == 1: \n",
    "    #     do something\n",
    "    \n",
    "    return # TODO\n",
    "def d_loss():\n",
    "    # if TASK == 1: \n",
    "    #     do something\n",
    "\n",
    "    return # TODO\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_true(100000)\n",
    "def iterate_minibatches(X, batchsize, y=None):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    for start in range(0, X.shape[0], batchsize):\n",
    "        end = min(start + batchsize, X.shape[0])\n",
    "        if y is None:\n",
    "            yield X[perm[start:end]]\n",
    "        else:\n",
    "            yield X[perm[start:end]], y[perm[start:end]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 12)\n",
    "vis_data(data)\n",
    "vis_g()\n",
    "vis_d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Legend**:\n",
    "- Blue dots are generated samples. \n",
    "- Colored histogram at the back shows density of real data. \n",
    "- And with arrows we show gradients of the discriminator -- they are the directions that discriminator pushes generator's samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# ===========================\n",
    "# IMPORTANT PARAMETER:\n",
    "# Number of D updates per G update\n",
    "# ===========================\n",
    "k_d, k_g = 4, 1\n",
    "\n",
    "accs = []\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        for input_data in iterate_minibatches(data, batch_size):\n",
    "            \n",
    "            # Optimize D\n",
    "            for _ in range(k_d):\n",
    "                # Sample noise\n",
    "                noise = Variable(torch.Tensor(sample_noise(len(input_data))))\n",
    "                \n",
    "                # Do an update\n",
    "                inp_data = Variable(torch.Tensor(input_data))\n",
    "                data_gen = generator(noise)\n",
    "                loss = d_loss(discriminator(data_gen), discriminator(inp_data))\n",
    "                d_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                d_optimizer.step()\n",
    "    \n",
    "            # Optimize G\n",
    "            for _ in range(k_g):\n",
    "                # Sample noise\n",
    "                noise = Variable(torch.Tensor(sample_noise(len(input_data))))\n",
    "                \n",
    "                # Do an update\n",
    "                data_gen = generator(noise)\n",
    "                loss = g_loss(discriminator(data_gen))\n",
    "                g_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                g_optimizer.step()\n",
    "            \n",
    "            # Visualize\n",
    "            plt.clf()\n",
    "            vis_data(data); vis_g(); vis_d()\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe your findings here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ya tomat. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
