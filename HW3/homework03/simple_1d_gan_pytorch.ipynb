{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__@this notebook__ will guide you through a very simple case of generative adversarial networks.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/model.png\" width=320px height=240px>\n",
    "\n",
    "Like.. veeery simple. Generative adversarial networks that learn to convert 1d uniform noise distribution to a uniform 1d normal data distribution.\n",
    "\n",
    "Unlike the main notebooks (`adversarial_*.ipynb`), this one features a lot of useful visualizations that will help you both get acquainted with the behavior of two networks and debug common errors without having to wait hours of CPU time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_noise(batch_size):\n",
    "    \"\"\" Uniform noise of shape [batch_size, 1] in range [0, 1]\"\"\"\n",
    "    return torch.rand(batch_size, 1)\n",
    "\n",
    "def sample_real_data(batch_size):\n",
    "    \"\"\" Normal noise of shape [batch_size, 1], mu=5, std=1.5 \"\"\"\n",
    "    return torch.randn(batch_size, 1) * 1.5 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator converts 1d noise into 1d data\n",
    "gen = nn.Sequential(nn.Linear(1, 16), nn.ELU(), nn.Linear(16, 1))\n",
    "gen_opt = torch.optim.SGD(gen.parameters(), lr=1e-3)\n",
    "\n",
    "# Discriminator converts 1d data into two logits (0th for real, 1st for fake). \n",
    "# It is deliberately made stronger than generator to make sure disc is slightly \"ahead in the game\".\n",
    "disc = nn.Sequential(nn.Linear(1, 64), nn.ELU(), nn.Linear(64, 2))\n",
    "disc_opt = torch.optim.SGD(disc.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we define 0-th output of discriminator as \"is_fake\" output and 1-st as \"is_real\"\n",
    "IS_FAKE, IS_REAL = 0, 1\n",
    "\n",
    "def train_disc(batch_size):\n",
    "    \"\"\" trains discriminator for one step \"\"\"\n",
    "    \n",
    "    # compute logp(real | x)\n",
    "    real_data = sample_real_data(batch_size)\n",
    "    logp_real_is_real = F.log_softmax(disc(real_data), dim=1)[:, IS_REAL]\n",
    "    \n",
    "    # compute logp(fake | G(z)). We detach to avoid computing gradinents through G(z)\n",
    "    noise = <sample noise>\n",
    "    gen_data = <generate data given noise>\n",
    "    logp_gen_is_fake = <compute logp for 0th>\n",
    "    \n",
    "    disc_loss = <compute loss>\n",
    "    \n",
    "    # sgd step. We zero_grad first to clear any gradients left from generator training\n",
    "    disc_opt.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "    return disc_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gen(batch_size):\n",
    "    \"\"\" trains generator for one step \"\"\"\n",
    "        \n",
    "    # compute logp(fake | G(z)).\n",
    "    noise = <sample noise>\n",
    "    gen_data = <generate data given noise>\n",
    "    \n",
    "    logp_gen_is_real = <compute logp gen is REAL>\n",
    "    \n",
    "    gen_loss = <generator loss>\n",
    "    \n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    return gen_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(100000):\n",
    "\n",
    "    for _ in range(5):\n",
    "        train_disc(128)\n",
    "    \n",
    "    train_gen(128)\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[14, 6])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Data distributions\")\n",
    "        plt.hist(gen(sample_noise(1000)).data.numpy()[:, 0],\n",
    "                 range=[0, 10], alpha=0.5, normed=True, label='gen')\n",
    "        plt.hist(sample_real_data(1000).data.numpy()[:,0],\n",
    "                 range=[0, 10], alpha=0.5, normed=True, label='real')\n",
    "        \n",
    "        x = np.linspace(0, 10, dtype='float32')\n",
    "        disc_preal = F.softmax(disc(torch.from_numpy(x[:, None])))[:, 1]\n",
    "        plt.plot(x, disc_preal.data.numpy(), label='disc P(real)')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Discriminator readout on real vs gen\")\n",
    "        plt.hist(F.softmax(disc(gen(sample_noise(100))))[:, 1].data.numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(G(z))')\n",
    "        plt.hist(F.softmax(disc(sample_real_data(100)))[:, 1].data.numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What to expect:__\n",
    "* __left:__ two distributions will start differently, but generator distribution should match real data _almost_ everywhere. The curve represents discriminator's opinion on all possible values of x. It should slowly get closer to 0.5 over areas where real data is dense.\n",
    "* __right:__ this chart shows how frequently does discriminator assign given probability to samples from real and generated data samples (shown in different colors). First several iterations will vary, but eventually they will both have nearly all probability mass around 0.5 as generator becomes better at it's job.\n",
    " * If instead it converges to two delta-functions around 0(gen) and 1(real) each, your discriminator has won. _Check generator loss function_. As a final measure, try decreasing discriminator learning rate. This can also happen if you replace mean over batch with sum or similar.\n",
    " * If it converges to 0.5 and stays there for several iterations but generator haven't learned to generate plausible data yet, generator is winning the game. _Double-check discriminator loss function_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
